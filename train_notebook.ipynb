{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.90 ðŸš€ Python-3.12.2 torch-2.4.0 CUDA:0 (NVIDIA GeForce GTX 1650, 3904MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=./data/YOLODataset/dataset.yaml, epochs=200, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.5, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/train7\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.head.Segment          [1, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 261 layers, 3,263,811 parameters, 3,263,795 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/labels/train.cache... 45 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/labels/val.cache... 11 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/segment/train7/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train7\u001b[0m\n",
      "Starting training for 200 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/200      2.85G      1.146      3.957      2.649      1.396         88        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25    0.00758          1      0.203      0.111    0.00697       0.92      0.229     0.0765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78f7d80d31a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/moein/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/moein/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1441, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/moein/anaconda3/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/moein/anaconda3/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/moein/anaconda3/lib/python3.12/multiprocessing/connection.py\", line 1135, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/moein/anaconda3/lib/python3.12/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/200      2.79G      1.266      4.106      2.619       1.52         99        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25    0.00758          1      0.187        0.1    0.00697       0.92      0.186     0.0679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/200      2.75G      1.145      4.027      2.653      1.428         83        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25    0.00758          1      0.638      0.427    0.00758          1       0.58      0.289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/200      2.73G     0.9345      2.301      2.169      1.266         73        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25    0.00758          1      0.586      0.394    0.00758          1      0.551      0.294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/200      2.76G     0.7538      1.482      1.496       1.08         90        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25       0.73      0.757      0.731      0.453       0.73      0.757      0.731       0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/200      2.75G     0.7853      1.445      1.202      1.118         93        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25      0.687       0.84      0.757      0.535      0.687       0.84      0.757      0.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/200      2.83G     0.7159      1.288      1.095      1.076         80        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25      0.706      0.961      0.802      0.552      0.706      0.961      0.802      0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/200      2.75G     0.7262      1.485      1.089      1.096         69        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25      0.708          1      0.795      0.589      0.708          1      0.795      0.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      9/200      2.77G     0.6767      1.155      1.031      1.077         64        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25      0.671       0.98      0.816      0.628      0.671       0.98      0.816      0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     10/200      2.82G     0.7419      1.495      1.064      1.107         73        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25      0.671       0.92       0.78      0.603      0.671       0.92       0.78      0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     11/200      2.82G     0.7031      1.131     0.9792      1.042         92        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25      0.713      0.895      0.834      0.592      0.713      0.895      0.834       0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     12/200       2.8G     0.7264      1.259     0.9971      1.072         71        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         11         25      0.703      0.947      0.877      0.614      0.694       0.96      0.858      0.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 3.12 MiB is free. Including non-PyTorch memory, this process has 2.30 GiB memory in use. Process 204987 has 1.50 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 88.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n-seg.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/YOLODataset/dataset.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m, degrees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, hsv_s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ultralytics/engine/model.py:815\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ultralytics/engine/trainer.py:208\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_train(world_size)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ultralytics/engine/trainer.py:386\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    385\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch)\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m world_size\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ultralytics/nn/tasks.py:105\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ultralytics/nn/tasks.py:286\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[0;32m--> 286\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(preds, batch)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ultralytics/nn/tasks.py:106\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ultralytics/nn/tasks.py:124\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ultralytics/nn/tasks.py:145\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 145\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    146\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ultralytics/nn/modules/head.py:176\u001b[0m, in \u001b[0;36mSegment.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m bs \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# batch size\u001b[39;00m\n\u001b[1;32m    175\u001b[0m mc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv4[i](x[i])\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnm, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl)], \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# mask coefficients\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m x \u001b[38;5;241m=\u001b[39m Detect\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m, x)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, mc, p\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ultralytics/nn/modules/head.py:57\u001b[0m, in \u001b[0;36mDetect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_end2end(x)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnl):\n\u001b[0;32m---> 57\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2[i](x[i]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3[i](x[i])), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/ultralytics/nn/modules/conv.py:50\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/activation.py:405\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:2104\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 2104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 3.12 MiB is free. Including non-PyTorch memory, this process has 2.30 GiB memory in use. Process 204987 has 1.50 GiB memory in use. Of the allocated memory 2.08 GiB is allocated by PyTorch, and 88.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "results = model.train(data=\"./data/YOLODataset/dataset.yaml\", epochs=200, imgsz=640, degrees=0.5, scale=0.5, hsv_s=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/11 /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/images/test/005.jpg: 512x640 (no detections), 41.4ms\n",
      "image 2/11 /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/images/test/013.jpg: 512x640 2 tooths, 33.7ms\n",
      "image 3/11 /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/images/test/019.jpg: 512x640 2 tooths, 32.2ms\n",
      "image 4/11 /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/images/test/030.jpg: 512x640 4 tooths, 33.7ms\n",
      "image 5/11 /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/images/test/034.jpg: 512x640 3 tooths, 35.4ms\n",
      "image 6/11 /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/images/test/036.jpg: 512x640 2 tooths, 25.8ms\n",
      "image 7/11 /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/images/test/046.jpg: 512x640 4 tooths, 30.3ms\n",
      "image 8/11 /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/images/test/049.jpg: 640x512 1 tooth, 28.9ms\n",
      "image 9/11 /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/images/test/050.jpg: 640x512 1 tooth, 32.5ms\n",
      "image 10/11 /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/images/test/056.jpg: 512x640 4 tooths, 32.2ms\n",
      "image 11/11 /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/images/test/066.jpg: 512x640 3 tooths, 36.2ms\n",
      "Speed: 4.1ms preprocess, 32.9ms inference, 4.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Results saved to \u001b[1mruns/segment/train6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(source='data/YOLODataset/images/test/', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = model.val(\n",
    "    data=\"./data/YOLODataset/dataset.yaml\", imgsz=640, batch=8, conf=0.25, iou=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.90 ðŸš€ Python-3.12.2 torch-2.4.0 CUDA:0 (NVIDIA GeForce GTX 1650, 3904MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/moein/Programming and Uni/Master - Amirkabir/Perio-detection/perio-detection/data/YOLODataset/labels/val.cache... 8 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          8         16      0.932          1       0.94      0.787      0.932          1       0.94       0.76\n",
      "Speed: 1.4ms preprocess, 57.8ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "metrics = model.val(data='./data/YOLODataset/dataset.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Masks object with attributes:\n",
       "\n",
       "data: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
       "orig_shape: (514, 664)\n",
       "shape: torch.Size([1, 512, 640])\n",
       "xy: [array([[     278.05,      72.325],\n",
       "       [     278.05,      75.438],\n",
       "       [      273.9,      79.588],\n",
       "       [      273.9,      80.625],\n",
       "       [     269.75,      84.775],\n",
       "       [     269.75,      85.812],\n",
       "       [     267.67,      87.887],\n",
       "       [     267.67,      88.925],\n",
       "       [     266.64,      89.963],\n",
       "       [     266.64,          91],\n",
       "       [     263.52,      94.113],\n",
       "       [     263.52,       95.15],\n",
       "       [     262.49,      96.188],\n",
       "       [     262.49,      98.262],\n",
       "       [     261.45,        99.3],\n",
       "       [     261.45,      100.34],\n",
       "       [     260.41,      101.38],\n",
       "       [     260.41,      102.41],\n",
       "       [     259.38,      103.45],\n",
       "       [     259.38,      110.71],\n",
       "       [     258.34,      111.75],\n",
       "       [     258.34,      166.74],\n",
       "       [     259.38,      167.77],\n",
       "       [     259.38,      168.81],\n",
       "       [     262.49,      171.93],\n",
       "       [     262.49,      172.96],\n",
       "       [     263.52,         174],\n",
       "       [     263.52,      175.04],\n",
       "       [     264.56,      176.07],\n",
       "       [     264.56,      178.15],\n",
       "       [      265.6,      179.19],\n",
       "       [      265.6,      183.34],\n",
       "       [     266.64,      184.38],\n",
       "       [     266.64,      195.79],\n",
       "       [      265.6,      196.82],\n",
       "       [      265.6,      205.12],\n",
       "       [     264.56,      206.16],\n",
       "       [     264.56,      213.43],\n",
       "       [     263.52,      214.46],\n",
       "       [     263.52,      220.69],\n",
       "       [     262.49,      221.72],\n",
       "       [     262.49,      233.14],\n",
       "       [     261.45,      234.18],\n",
       "       [     261.45,      238.32],\n",
       "       [     260.41,      239.36],\n",
       "       [     260.41,      241.44],\n",
       "       [     259.38,      242.47],\n",
       "       [     259.38,      246.62],\n",
       "       [     258.34,      247.66],\n",
       "       [     258.34,      250.77],\n",
       "       [      257.3,      251.81],\n",
       "       [      257.3,      253.89],\n",
       "       [     256.26,      254.93],\n",
       "       [     256.26,         257],\n",
       "       [     250.04,      263.23],\n",
       "       [     250.04,      264.26],\n",
       "       [     246.92,      267.38],\n",
       "       [     246.92,      269.45],\n",
       "       [     245.89,      270.49],\n",
       "       [     245.89,      271.52],\n",
       "       [     243.81,       273.6],\n",
       "       [     243.81,      274.64],\n",
       "       [     242.77,      275.67],\n",
       "       [     242.77,      276.71],\n",
       "       [     241.74,      277.75],\n",
       "       [     241.74,      279.82],\n",
       "       [     239.66,       281.9],\n",
       "       [     239.66,      282.94],\n",
       "       [     238.62,      283.98],\n",
       "       [     238.62,      286.05],\n",
       "       [     237.59,      287.09],\n",
       "       [     237.59,      289.16],\n",
       "       [     236.55,       290.2],\n",
       "       [     236.55,      291.24],\n",
       "       [     235.51,      292.27],\n",
       "       [     235.51,      293.31],\n",
       "       [     234.47,      294.35],\n",
       "       [     234.47,      295.39],\n",
       "       [     233.44,      296.42],\n",
       "       [     233.44,       298.5],\n",
       "       [      232.4,      299.54],\n",
       "       [      232.4,      300.57],\n",
       "       [     231.36,      301.61],\n",
       "       [     231.36,      302.65],\n",
       "       [     230.32,      303.69],\n",
       "       [     230.32,      305.76],\n",
       "       [     229.29,       306.8],\n",
       "       [     229.29,      307.84],\n",
       "       [     228.25,      308.88],\n",
       "       [     228.25,      309.91],\n",
       "       [     226.17,      311.99],\n",
       "       [     226.17,      314.06],\n",
       "       [     225.14,       315.1],\n",
       "       [     225.14,      317.17],\n",
       "       [      224.1,      318.21],\n",
       "       [      224.1,      320.29],\n",
       "       [     223.06,      321.32],\n",
       "       [     223.06,      324.44],\n",
       "       [     222.02,      325.48],\n",
       "       [     222.02,      328.59],\n",
       "       [     220.99,      329.62],\n",
       "       [     220.99,      332.74],\n",
       "       [     219.95,      333.77],\n",
       "       [     219.95,         340],\n",
       "       [     220.99,      341.04],\n",
       "       [     220.99,      349.34],\n",
       "       [     222.02,      350.38],\n",
       "       [     222.02,      359.71],\n",
       "       [     220.99,      360.75],\n",
       "       [     220.99,      372.16],\n",
       "       [     219.95,       373.2],\n",
       "       [     219.95,      379.42],\n",
       "       [     218.91,      380.46],\n",
       "       [     218.91,      385.65],\n",
       "       [     217.88,      386.69],\n",
       "       [     217.88,      387.73],\n",
       "       [     218.91,      388.76],\n",
       "       [     218.91,      390.84],\n",
       "       [     219.95,      391.88],\n",
       "       [     219.95,      394.99],\n",
       "       [     220.99,      396.02],\n",
       "       [     220.99,      397.06],\n",
       "       [     222.02,       398.1],\n",
       "       [     222.02,      403.29],\n",
       "       [     225.14,       406.4],\n",
       "       [     234.47,       406.4],\n",
       "       [     235.51,      405.36],\n",
       "       [     236.55,      405.36],\n",
       "       [     237.59,      404.32],\n",
       "       [     239.66,      404.32],\n",
       "       [      240.7,      403.29],\n",
       "       [     241.74,      403.29],\n",
       "       [     242.77,      402.25],\n",
       "       [        249,      402.25],\n",
       "       [     250.04,      403.29],\n",
       "       [     254.19,      403.29],\n",
       "       [     255.22,      404.32],\n",
       "       [      257.3,      404.32],\n",
       "       [     258.34,      405.36],\n",
       "       [     264.56,      405.36],\n",
       "       [      265.6,       406.4],\n",
       "       [     266.64,       406.4],\n",
       "       [     271.82,      411.59],\n",
       "       [     272.86,      411.59],\n",
       "       [     274.94,      413.66],\n",
       "       [     275.98,      413.66],\n",
       "       [     283.24,      420.92],\n",
       "       [     284.27,      420.92],\n",
       "       [     289.46,      426.11],\n",
       "       [     289.46,      427.15],\n",
       "       [     291.54,      429.23],\n",
       "       [     291.54,      432.34],\n",
       "       [        332,      432.34],\n",
       "       [        332,      428.19],\n",
       "       [     334.07,      426.11],\n",
       "       [     334.07,      425.07],\n",
       "       [     335.11,      424.04],\n",
       "       [     335.11,         423],\n",
       "       [     336.15,      421.96],\n",
       "       [     336.15,      420.92],\n",
       "       [     337.19,      419.89],\n",
       "       [     337.19,      418.85],\n",
       "       [     339.26,      416.77],\n",
       "       [     339.26,      415.74],\n",
       "       [     342.38,      412.62],\n",
       "       [     342.38,      410.55],\n",
       "       [     343.41,      409.51],\n",
       "       [     343.41,      407.44],\n",
       "       [     344.45,       406.4],\n",
       "       [     344.45,      404.32],\n",
       "       [     345.49,      403.29],\n",
       "       [     345.49,      401.21],\n",
       "       [     347.56,      399.14],\n",
       "       [     347.56,       398.1],\n",
       "       [      348.6,      397.06],\n",
       "       [      348.6,      396.02],\n",
       "       [     349.64,      394.99],\n",
       "       [     349.64,      393.95],\n",
       "       [     351.71,      391.88],\n",
       "       [     351.71,      390.84],\n",
       "       [     352.75,       389.8],\n",
       "       [     352.75,      388.76],\n",
       "       [     353.79,      387.73],\n",
       "       [     353.79,      386.69],\n",
       "       [     354.82,      385.65],\n",
       "       [     354.82,      384.61],\n",
       "       [     355.86,      383.57],\n",
       "       [     355.86,      382.54],\n",
       "       [      356.9,       381.5],\n",
       "       [      356.9,      380.46],\n",
       "       [     357.94,      379.42],\n",
       "       [     357.94,      376.31],\n",
       "       [     358.98,      375.27],\n",
       "       [     358.98,      374.24],\n",
       "       [     360.01,       373.2],\n",
       "       [     360.01,      372.16],\n",
       "       [     361.05,      371.12],\n",
       "       [     361.05,      369.05],\n",
       "       [     362.09,      368.01],\n",
       "       [     362.09,      366.98],\n",
       "       [     363.12,      365.94],\n",
       "       [     363.12,       364.9],\n",
       "       [     364.16,      363.86],\n",
       "       [     364.16,      362.82],\n",
       "       [      365.2,      361.79],\n",
       "       [      365.2,      358.67],\n",
       "       [     366.24,      357.64],\n",
       "       [     366.24,       356.6],\n",
       "       [     367.27,      355.56],\n",
       "       [     367.27,      354.52],\n",
       "       [     368.31,      353.49],\n",
       "       [     368.31,      352.45],\n",
       "       [     369.35,      351.41],\n",
       "       [     369.35,      349.34],\n",
       "       [     370.39,       348.3],\n",
       "       [     370.39,      347.26],\n",
       "       [     371.42,      346.23],\n",
       "       [     371.42,      345.19],\n",
       "       [     372.46,      344.15],\n",
       "       [     372.46,      342.07],\n",
       "       [      373.5,      341.04],\n",
       "       [      373.5,      336.89],\n",
       "       [     374.54,      335.85],\n",
       "       [     374.54,       331.7],\n",
       "       [     375.57,      330.66],\n",
       "       [     375.57,      327.55],\n",
       "       [     376.61,      326.51],\n",
       "       [     376.61,       323.4],\n",
       "       [     377.65,      322.36],\n",
       "       [     377.65,      319.25],\n",
       "       [     378.69,      318.21],\n",
       "       [     378.69,      316.14],\n",
       "       [     379.72,       315.1],\n",
       "       [     379.72,      313.02],\n",
       "       [     380.76,      311.99],\n",
       "       [     380.76,      308.88],\n",
       "       [      381.8,      307.84],\n",
       "       [      381.8,      305.76],\n",
       "       [     382.84,      304.73],\n",
       "       [     382.84,      302.65],\n",
       "       [     383.88,      301.61],\n",
       "       [     383.88,      299.54],\n",
       "       [     384.91,       298.5],\n",
       "       [     384.91,      297.46],\n",
       "       [     385.95,      296.42],\n",
       "       [     385.95,      295.39],\n",
       "       [     386.99,      294.35],\n",
       "       [     386.99,      292.27],\n",
       "       [     388.02,      291.24],\n",
       "       [     388.02,       290.2],\n",
       "       [     389.06,      289.16],\n",
       "       [     389.06,      286.05],\n",
       "       [      390.1,      285.01],\n",
       "       [      390.1,       281.9],\n",
       "       [     391.14,      280.86],\n",
       "       [     391.14,      277.75],\n",
       "       [     392.17,      276.71],\n",
       "       [     392.17,      274.64],\n",
       "       [     393.21,       273.6],\n",
       "       [     393.21,      270.49],\n",
       "       [     394.25,      269.45],\n",
       "       [     394.25,      267.38],\n",
       "       [     395.29,      266.34],\n",
       "       [     395.29,      264.26],\n",
       "       [     396.32,      263.23],\n",
       "       [     396.32,      261.15],\n",
       "       [     397.36,      260.11],\n",
       "       [     397.36,      258.04],\n",
       "       [      398.4,         257],\n",
       "       [      398.4,      254.93],\n",
       "       [     399.44,      253.89],\n",
       "       [     399.44,      252.85],\n",
       "       [     400.47,      251.81],\n",
       "       [     400.47,      249.74],\n",
       "       [     402.55,      247.66],\n",
       "       [     402.55,      246.62],\n",
       "       [     403.59,      245.59],\n",
       "       [     403.59,      243.51],\n",
       "       [     404.62,      242.47],\n",
       "       [     404.62,      238.32],\n",
       "       [     405.66,      237.29],\n",
       "       [     405.66,       232.1],\n",
       "       [      406.7,      231.06],\n",
       "       [      406.7,      219.65],\n",
       "       [     407.74,      218.61],\n",
       "       [     407.74,      216.54],\n",
       "       [     408.77,       215.5],\n",
       "       [     408.77,      214.46],\n",
       "       [     409.81,      213.43],\n",
       "       [     409.81,      211.35],\n",
       "       [     410.85,      210.31],\n",
       "       [     410.85,      195.79],\n",
       "       [     411.89,      194.75],\n",
       "       [     411.89,      193.71],\n",
       "       [     412.92,      192.68],\n",
       "       [     412.92,      191.64],\n",
       "       [        415,      189.56],\n",
       "       [        415,      188.52],\n",
       "       [     416.04,      187.49],\n",
       "       [     416.04,      186.45],\n",
       "       [     417.07,      185.41],\n",
       "       [     417.07,      183.34],\n",
       "       [     418.11,       182.3],\n",
       "       [     418.11,      179.19],\n",
       "       [     419.15,      178.15],\n",
       "       [     419.15,      176.07],\n",
       "       [     420.19,      175.04],\n",
       "       [     420.19,      169.85],\n",
       "       [     421.22,      168.81],\n",
       "       [     424.34,      168.81],\n",
       "       [     424.34,      163.62],\n",
       "       [     421.22,      163.62],\n",
       "       [     420.19,      162.59],\n",
       "       [     420.19,      159.48],\n",
       "       [     419.15,      158.44],\n",
       "       [     419.15,       157.4],\n",
       "       [     418.11,      156.36],\n",
       "       [     418.11,      154.29],\n",
       "       [     417.07,      153.25],\n",
       "       [     417.07,      151.18],\n",
       "       [     416.04,      150.14],\n",
       "       [     416.04,       149.1],\n",
       "       [        415,      148.06],\n",
       "       [        415,      147.02],\n",
       "       [     412.92,      144.95],\n",
       "       [     412.92,      143.91],\n",
       "       [      406.7,      137.69],\n",
       "       [      406.7,      136.65],\n",
       "       [     404.62,      134.57],\n",
       "       [     404.62,      131.46],\n",
       "       [     403.59,      130.43],\n",
       "       [     403.59,      127.31],\n",
       "       [     402.55,      126.28],\n",
       "       [     402.55,      125.24],\n",
       "       [     401.51,       124.2],\n",
       "       [     401.51,      122.12],\n",
       "       [     399.44,      120.05],\n",
       "       [     399.44,      119.01],\n",
       "       [     396.32,       115.9],\n",
       "       [     396.32,      114.86],\n",
       "       [     394.25,      112.79],\n",
       "       [     394.25,      111.75],\n",
       "       [     391.14,      108.64],\n",
       "       [     391.14,       107.6],\n",
       "       [     384.91,      101.38],\n",
       "       [     384.91,      100.34],\n",
       "       [     383.88,      100.34],\n",
       "       [     380.76,      97.225],\n",
       "       [     379.72,      97.225],\n",
       "       [     378.69,      96.188],\n",
       "       [     378.69,       95.15],\n",
       "       [     377.65,      94.113],\n",
       "       [     376.61,      94.113],\n",
       "       [     371.42,      88.925],\n",
       "       [     370.39,      88.925],\n",
       "       [     369.35,      87.887],\n",
       "       [     369.35,       86.85],\n",
       "       [     368.31,       86.85],\n",
       "       [      365.2,      83.738],\n",
       "       [     364.16,      83.738],\n",
       "       [     362.09,      81.662],\n",
       "       [     361.05,      81.662],\n",
       "       [     358.98,      79.588],\n",
       "       [     357.94,      79.588],\n",
       "       [      356.9,       78.55],\n",
       "       [     354.82,       78.55],\n",
       "       [     353.79,      77.513],\n",
       "       [     350.67,      77.513],\n",
       "       [      348.6,      75.438],\n",
       "       [      348.6,      72.325]], dtype=float32)]\n",
       "xyn: [array([[    0.41875,     0.14071],\n",
       "       [    0.41875,     0.14677],\n",
       "       [     0.4125,     0.15484],\n",
       "       [     0.4125,     0.15686],\n",
       "       [    0.40625,     0.16493],\n",
       "       [    0.40625,     0.16695],\n",
       "       [    0.40312,     0.17099],\n",
       "       [    0.40312,     0.17301],\n",
       "       [    0.40156,     0.17502],\n",
       "       [    0.40156,     0.17704],\n",
       "       [    0.39687,      0.1831],\n",
       "       [    0.39687,     0.18512],\n",
       "       [    0.39531,     0.18714],\n",
       "       [    0.39531,     0.19117],\n",
       "       [    0.39375,     0.19319],\n",
       "       [    0.39375,     0.19521],\n",
       "       [    0.39219,     0.19723],\n",
       "       [    0.39219,     0.19925],\n",
       "       [    0.39062,     0.20126],\n",
       "       [    0.39062,     0.21539],\n",
       "       [    0.38906,     0.21741],\n",
       "       [    0.38906,     0.32439],\n",
       "       [    0.39062,     0.32641],\n",
       "       [    0.39062,     0.32843],\n",
       "       [    0.39531,     0.33448],\n",
       "       [    0.39531,      0.3365],\n",
       "       [    0.39687,     0.33852],\n",
       "       [    0.39687,     0.34054],\n",
       "       [    0.39844,     0.34256],\n",
       "       [    0.39844,      0.3466],\n",
       "       [        0.4,     0.34861],\n",
       "       [        0.4,     0.35669],\n",
       "       [    0.40156,     0.35871],\n",
       "       [    0.40156,     0.38091],\n",
       "       [        0.4,     0.38293],\n",
       "       [        0.4,     0.39908],\n",
       "       [    0.39844,     0.40109],\n",
       "       [    0.39844,     0.41522],\n",
       "       [    0.39687,     0.41724],\n",
       "       [    0.39687,     0.42935],\n",
       "       [    0.39531,     0.43137],\n",
       "       [    0.39531,     0.45357],\n",
       "       [    0.39375,     0.45559],\n",
       "       [    0.39375,     0.46367],\n",
       "       [    0.39219,     0.46569],\n",
       "       [    0.39219,     0.46972],\n",
       "       [    0.39062,     0.47174],\n",
       "       [    0.39062,     0.47982],\n",
       "       [    0.38906,     0.48183],\n",
       "       [    0.38906,     0.48789],\n",
       "       [     0.3875,     0.48991],\n",
       "       [     0.3875,     0.49394],\n",
       "       [    0.38594,     0.49596],\n",
       "       [    0.38594,         0.5],\n",
       "       [    0.37656,     0.51211],\n",
       "       [    0.37656,     0.51413],\n",
       "       [    0.37187,     0.52018],\n",
       "       [    0.37187,     0.52422],\n",
       "       [    0.37031,     0.52624],\n",
       "       [    0.37031,     0.52826],\n",
       "       [    0.36719,      0.5323],\n",
       "       [    0.36719,     0.53431],\n",
       "       [    0.36562,     0.53633],\n",
       "       [    0.36562,     0.53835],\n",
       "       [    0.36406,     0.54037],\n",
       "       [    0.36406,     0.54441],\n",
       "       [    0.36094,     0.54844],\n",
       "       [    0.36094,     0.55046],\n",
       "       [    0.35938,     0.55248],\n",
       "       [    0.35938,     0.55652],\n",
       "       [    0.35781,     0.55854],\n",
       "       [    0.35781,     0.56257],\n",
       "       [    0.35625,     0.56459],\n",
       "       [    0.35625,     0.56661],\n",
       "       [    0.35469,     0.56863],\n",
       "       [    0.35469,     0.57065],\n",
       "       [    0.35312,     0.57267],\n",
       "       [    0.35312,     0.57468],\n",
       "       [    0.35156,      0.5767],\n",
       "       [    0.35156,     0.58074],\n",
       "       [       0.35,     0.58276],\n",
       "       [       0.35,     0.58478],\n",
       "       [    0.34844,     0.58679],\n",
       "       [    0.34844,     0.58881],\n",
       "       [    0.34687,     0.59083],\n",
       "       [    0.34687,     0.59487],\n",
       "       [    0.34531,     0.59689],\n",
       "       [    0.34531,     0.59891],\n",
       "       [    0.34375,     0.60092],\n",
       "       [    0.34375,     0.60294],\n",
       "       [    0.34062,     0.60698],\n",
       "       [    0.34062,     0.61102],\n",
       "       [    0.33906,     0.61304],\n",
       "       [    0.33906,     0.61707],\n",
       "       [     0.3375,     0.61909],\n",
       "       [     0.3375,     0.62313],\n",
       "       [    0.33594,     0.62515],\n",
       "       [    0.33594,      0.6312],\n",
       "       [    0.33437,     0.63322],\n",
       "       [    0.33437,     0.63928],\n",
       "       [    0.33281,     0.64129],\n",
       "       [    0.33281,     0.64735],\n",
       "       [    0.33125,     0.64937],\n",
       "       [    0.33125,     0.66148],\n",
       "       [    0.33281,      0.6635],\n",
       "       [    0.33281,     0.67964],\n",
       "       [    0.33437,     0.68166],\n",
       "       [    0.33437,     0.69983],\n",
       "       [    0.33281,     0.70185],\n",
       "       [    0.33281,     0.72405],\n",
       "       [    0.33125,     0.72607],\n",
       "       [    0.33125,     0.73818],\n",
       "       [    0.32969,      0.7402],\n",
       "       [    0.32969,     0.75029],\n",
       "       [    0.32812,     0.75231],\n",
       "       [    0.32812,     0.75433],\n",
       "       [    0.32969,     0.75635],\n",
       "       [    0.32969,     0.76038],\n",
       "       [    0.33125,      0.7624],\n",
       "       [    0.33125,     0.76846],\n",
       "       [    0.33281,     0.77048],\n",
       "       [    0.33281,      0.7725],\n",
       "       [    0.33437,     0.77451],\n",
       "       [    0.33437,     0.78461],\n",
       "       [    0.33906,     0.79066],\n",
       "       [    0.35312,     0.79066],\n",
       "       [    0.35469,     0.78864],\n",
       "       [    0.35625,     0.78864],\n",
       "       [    0.35781,     0.78662],\n",
       "       [    0.36094,     0.78662],\n",
       "       [     0.3625,     0.78461],\n",
       "       [    0.36406,     0.78461],\n",
       "       [    0.36562,     0.78259],\n",
       "       [      0.375,     0.78259],\n",
       "       [    0.37656,     0.78461],\n",
       "       [    0.38281,     0.78461],\n",
       "       [    0.38437,     0.78662],\n",
       "       [     0.3875,     0.78662],\n",
       "       [    0.38906,     0.78864],\n",
       "       [    0.39844,     0.78864],\n",
       "       [        0.4,     0.79066],\n",
       "       [    0.40156,     0.79066],\n",
       "       [    0.40937,     0.80075],\n",
       "       [    0.41094,     0.80075],\n",
       "       [    0.41406,     0.80479],\n",
       "       [    0.41563,     0.80479],\n",
       "       [    0.42656,     0.81892],\n",
       "       [    0.42812,     0.81892],\n",
       "       [    0.43594,     0.82901],\n",
       "       [    0.43594,     0.83103],\n",
       "       [    0.43906,     0.83507],\n",
       "       [    0.43906,     0.84112],\n",
       "       [        0.5,     0.84112],\n",
       "       [        0.5,     0.83305],\n",
       "       [    0.50312,     0.82901],\n",
       "       [    0.50312,     0.82699],\n",
       "       [    0.50469,     0.82498],\n",
       "       [    0.50469,     0.82296],\n",
       "       [    0.50625,     0.82094],\n",
       "       [    0.50625,     0.81892],\n",
       "       [    0.50781,      0.8169],\n",
       "       [    0.50781,     0.81488],\n",
       "       [    0.51094,     0.81085],\n",
       "       [    0.51094,     0.80883],\n",
       "       [    0.51562,     0.80277],\n",
       "       [    0.51562,     0.79874],\n",
       "       [    0.51719,     0.79672],\n",
       "       [    0.51719,     0.79268],\n",
       "       [    0.51875,     0.79066],\n",
       "       [    0.51875,     0.78662],\n",
       "       [    0.52031,     0.78461],\n",
       "       [    0.52031,     0.78057],\n",
       "       [    0.52344,     0.77653],\n",
       "       [    0.52344,     0.77451],\n",
       "       [      0.525,      0.7725],\n",
       "       [      0.525,     0.77048],\n",
       "       [    0.52656,     0.76846],\n",
       "       [    0.52656,     0.76644],\n",
       "       [    0.52969,      0.7624],\n",
       "       [    0.52969,     0.76038],\n",
       "       [    0.53125,     0.75837],\n",
       "       [    0.53125,     0.75635],\n",
       "       [    0.53281,     0.75433],\n",
       "       [    0.53281,     0.75231],\n",
       "       [    0.53437,     0.75029],\n",
       "       [    0.53437,     0.74827],\n",
       "       [    0.53594,     0.74625],\n",
       "       [    0.53594,     0.74424],\n",
       "       [     0.5375,     0.74222],\n",
       "       [     0.5375,      0.7402],\n",
       "       [    0.53906,     0.73818],\n",
       "       [    0.53906,     0.73213],\n",
       "       [    0.54063,     0.73011],\n",
       "       [    0.54063,     0.72809],\n",
       "       [    0.54219,     0.72607],\n",
       "       [    0.54219,     0.72405],\n",
       "       [    0.54375,     0.72203],\n",
       "       [    0.54375,       0.718],\n",
       "       [    0.54531,     0.71598],\n",
       "       [    0.54531,     0.71396],\n",
       "       [    0.54688,     0.71194],\n",
       "       [    0.54688,     0.70992],\n",
       "       [    0.54844,      0.7079],\n",
       "       [    0.54844,     0.70589],\n",
       "       [       0.55,     0.70387],\n",
       "       [       0.55,     0.69781],\n",
       "       [    0.55156,     0.69579],\n",
       "       [    0.55156,     0.69377],\n",
       "       [    0.55312,     0.69176],\n",
       "       [    0.55312,     0.68974],\n",
       "       [    0.55469,     0.68772],\n",
       "       [    0.55469,      0.6857],\n",
       "       [    0.55625,     0.68368],\n",
       "       [    0.55625,     0.67964],\n",
       "       [    0.55781,     0.67763],\n",
       "       [    0.55781,     0.67561],\n",
       "       [    0.55937,     0.67359],\n",
       "       [    0.55937,     0.67157],\n",
       "       [    0.56094,     0.66955],\n",
       "       [    0.56094,     0.66552],\n",
       "       [     0.5625,      0.6635],\n",
       "       [     0.5625,     0.65542],\n",
       "       [    0.56406,      0.6534],\n",
       "       [    0.56406,     0.64533],\n",
       "       [    0.56562,     0.64331],\n",
       "       [    0.56562,     0.63726],\n",
       "       [    0.56719,     0.63524],\n",
       "       [    0.56719,     0.62918],\n",
       "       [    0.56875,     0.62716],\n",
       "       [    0.56875,     0.62111],\n",
       "       [    0.57031,     0.61909],\n",
       "       [    0.57031,     0.61505],\n",
       "       [    0.57187,     0.61304],\n",
       "       [    0.57187,       0.609],\n",
       "       [    0.57344,     0.60698],\n",
       "       [    0.57344,     0.60092],\n",
       "       [      0.575,     0.59891],\n",
       "       [      0.575,     0.59487],\n",
       "       [    0.57656,     0.59285],\n",
       "       [    0.57656,     0.58881],\n",
       "       [    0.57812,     0.58679],\n",
       "       [    0.57812,     0.58276],\n",
       "       [    0.57969,     0.58074],\n",
       "       [    0.57969,     0.57872],\n",
       "       [    0.58125,      0.5767],\n",
       "       [    0.58125,     0.57468],\n",
       "       [    0.58281,     0.57267],\n",
       "       [    0.58281,     0.56863],\n",
       "       [    0.58437,     0.56661],\n",
       "       [    0.58437,     0.56459],\n",
       "       [    0.58594,     0.56257],\n",
       "       [    0.58594,     0.55652],\n",
       "       [     0.5875,      0.5545],\n",
       "       [     0.5875,     0.54844],\n",
       "       [    0.58906,     0.54643],\n",
       "       [    0.58906,     0.54037],\n",
       "       [    0.59062,     0.53835],\n",
       "       [    0.59062,     0.53431],\n",
       "       [    0.59219,      0.5323],\n",
       "       [    0.59219,     0.52624],\n",
       "       [    0.59375,     0.52422],\n",
       "       [    0.59375,     0.52018],\n",
       "       [    0.59531,     0.51817],\n",
       "       [    0.59531,     0.51413],\n",
       "       [    0.59687,     0.51211],\n",
       "       [    0.59687,     0.50807],\n",
       "       [    0.59844,     0.50606],\n",
       "       [    0.59844,     0.50202],\n",
       "       [        0.6,         0.5],\n",
       "       [        0.6,     0.49596],\n",
       "       [    0.60156,     0.49394],\n",
       "       [    0.60156,     0.49193],\n",
       "       [    0.60312,     0.48991],\n",
       "       [    0.60312,     0.48587],\n",
       "       [    0.60625,     0.48183],\n",
       "       [    0.60625,     0.47982],\n",
       "       [    0.60781,      0.4778],\n",
       "       [    0.60781,     0.47376],\n",
       "       [    0.60938,     0.47174],\n",
       "       [    0.60938,     0.46367],\n",
       "       [    0.61094,     0.46165],\n",
       "       [    0.61094,     0.45156],\n",
       "       [     0.6125,     0.44954],\n",
       "       [     0.6125,     0.42733],\n",
       "       [    0.61406,     0.42532],\n",
       "       [    0.61406,     0.42128],\n",
       "       [    0.61562,     0.41926],\n",
       "       [    0.61562,     0.41724],\n",
       "       [    0.61719,     0.41522],\n",
       "       [    0.61719,     0.41119],\n",
       "       [    0.61875,     0.40917],\n",
       "       [    0.61875,     0.38091],\n",
       "       [    0.62031,     0.37889],\n",
       "       [    0.62031,     0.37687],\n",
       "       [    0.62187,     0.37485],\n",
       "       [    0.62187,     0.37284],\n",
       "       [      0.625,      0.3688],\n",
       "       [      0.625,     0.36678],\n",
       "       [    0.62656,     0.36476],\n",
       "       [    0.62656,     0.36274],\n",
       "       [    0.62812,     0.36072],\n",
       "       [    0.62812,     0.35669],\n",
       "       [    0.62969,     0.35467],\n",
       "       [    0.62969,     0.34861],\n",
       "       [    0.63125,      0.3466],\n",
       "       [    0.63125,     0.34256],\n",
       "       [    0.63281,     0.34054],\n",
       "       [    0.63281,     0.33045],\n",
       "       [    0.63437,     0.32843],\n",
       "       [    0.63906,     0.32843],\n",
       "       [    0.63906,     0.31834],\n",
       "       [    0.63437,     0.31834],\n",
       "       [    0.63281,     0.31632],\n",
       "       [    0.63281,     0.31026],\n",
       "       [    0.63125,     0.30824],\n",
       "       [    0.63125,     0.30623],\n",
       "       [    0.62969,     0.30421],\n",
       "       [    0.62969,     0.30017],\n",
       "       [    0.62812,     0.29815],\n",
       "       [    0.62812,     0.29411],\n",
       "       [    0.62656,      0.2921],\n",
       "       [    0.62656,     0.29008],\n",
       "       [      0.625,     0.28806],\n",
       "       [      0.625,     0.28604],\n",
       "       [    0.62187,       0.282],\n",
       "       [    0.62187,     0.27999],\n",
       "       [     0.6125,     0.26787],\n",
       "       [     0.6125,     0.26586],\n",
       "       [    0.60938,     0.26182],\n",
       "       [    0.60938,     0.25576],\n",
       "       [    0.60781,     0.25375],\n",
       "       [    0.60781,     0.24769],\n",
       "       [    0.60625,     0.24567],\n",
       "       [    0.60625,     0.24365],\n",
       "       [    0.60469,     0.24163],\n",
       "       [    0.60469,      0.2376],\n",
       "       [    0.60156,     0.23356],\n",
       "       [    0.60156,     0.23154],\n",
       "       [    0.59687,     0.22549],\n",
       "       [    0.59687,     0.22347],\n",
       "       [    0.59375,     0.21943],\n",
       "       [    0.59375,     0.21741],\n",
       "       [    0.58906,     0.21136],\n",
       "       [    0.58906,     0.20934],\n",
       "       [    0.57969,     0.19723],\n",
       "       [    0.57969,     0.19521],\n",
       "       [    0.57812,     0.19521],\n",
       "       [    0.57344,     0.18915],\n",
       "       [    0.57187,     0.18915],\n",
       "       [    0.57031,     0.18714],\n",
       "       [    0.57031,     0.18512],\n",
       "       [    0.56875,      0.1831],\n",
       "       [    0.56719,      0.1831],\n",
       "       [    0.55937,     0.17301],\n",
       "       [    0.55781,     0.17301],\n",
       "       [    0.55625,     0.17099],\n",
       "       [    0.55625,     0.16897],\n",
       "       [    0.55469,     0.16897],\n",
       "       [       0.55,     0.16291],\n",
       "       [    0.54844,     0.16291],\n",
       "       [    0.54531,     0.15888],\n",
       "       [    0.54375,     0.15888],\n",
       "       [    0.54063,     0.15484],\n",
       "       [    0.53906,     0.15484],\n",
       "       [     0.5375,     0.15282],\n",
       "       [    0.53437,     0.15282],\n",
       "       [    0.53281,      0.1508],\n",
       "       [    0.52812,      0.1508],\n",
       "       [      0.525,     0.14677],\n",
       "       [      0.525,     0.14071]], dtype=float32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
